---

# Dataset metadata
default_dataset: &default_dataset
    type: PythonDataset
    delim: "\t"
    buffer_size: 47800
    shuffle: True
    mask: 2
    map_tables:
        token2id:
            path: data/wordseg/char_8k.vocab
        tag2id:
            path: data/wordseg/sighan05_pku/label2id.dict

    slots: 
        tokens: &default_slot
            index: 0
            type: sequence
            delim: " "
            map_table: token2id
            max_length: 50
            pad: 0
        tags:
            index: 1
            map_table: tag2id
            << : *default_slot
        nwords:
            index: 2
            type: value
            value_type: int64

pred_dataset: &pred_dataset
    type: PythonDataset
    delim: "\t"
    buffer_size: 47800
    shuffle: False
    mask: 2
    map_tables:
        token2id:
            path: data/wordseg/char_8k.vocab
        tag2id:
            path: data/wordseg/sighan05_pku/label2id.dict

    slots: 
        tokens:
            << : *default_slot
        tags: 
            << : *default_slot
        nwords:
            index: 1
            type: value
            value_type: int64

train_dataset: &train_dataset
    path: data/wordseg/sighan05_pku/train.data.short
    << : *default_dataset

dev_dataset: &dev_dataset
    shuffle: False
    path: data/wordseg/sighan05_pku/test.data.short
    << : *default_dataset

test_dataset: &test_dataset
    shuffle: False
    path: data/wordseg/sighan05_pku/test.data.short
    << : *default_dataset

predict_dataset: &predict_dataset
    shuffle: False
    path: data/wordseg/sighan05_pku/test.data.short
    << : *pred_dataset

# logging:
    # file: results/wordseg_pku/region_emb_128_9_cw_short_global/logging.out
    # level: 3

# Model config
model:
    model_name: GlobalContextRegionChunkingModel
    use_word_pretrain_emb: False
    # word_emb_finetune: False
    # word2vec_dict: ''
    use_pretrain: False
    pretrain_finetune: False

    vocab_size : 8003

    use_crf: True
    max_length: 50
    emb_size: 128
    region_size: 9
    channel_size: 512
    num_layers: 1
    aggregate_method_within_region: attention
    FC_between_layer: True
    n_alpha: 1

    use_lstm: True
    hidden_size: 256

    batch_size: 400
    optimizer: Adam
    learning_rate: 0.005
    use_clip_by_norm: True
    decay_step: 400
    decay_rate: 1.0
    n_classes: 6
    dropout_rate: 0.2
    metric: NERMetric

# Estimator
estimator:
    type: PythonSequenceLabellingEstimator
    train_dataset: *train_dataset
    eval_datasets:
        # - *train_dataset
        - *dev_dataset
        # - *test_dataset
    eval_op_path: results/wordseg_pku/region_emb_128_9_cw_short_global_lstm/summary/eval.output
    # infer_dataset: *predict_dataset
    # infer_op_path: results/wordseg_pku/region_emb_128_9_cw_short_global/summary/infer.output

    batch_size: 400
    max_epochs: 40

    use_crf: True
    chunking: True
    eval_with_input: True
    use_entity_f1: True
    word2id: data/wordseg/char_8k.vocab
    label2id: data/wordseg/sighan05_pku/label2id.dict
    infer2id: data/wordseg/sighan05_pku/infer2id.dict
    infer2id_ent: data/wordseg/sighan05_pku/infer2id_ent.dict
    checkpoint_dir: results/wordseg_pku/region_emb_128_9_cw_short_global_lstm

    model_name: region_sequence_labbelling_model
    # save_checkpoints_steps: 2000
    # auto_end_time: 1000
    eval_interval_steps: 100
    # max_training_steps: 20000
    log_every_n_steps: 100
    display_eval: True
    tolerance: 5
    save_eval_to_file: False

    # save_best_result: True
    # best_checkpoint_dir: results/wordseg_pku/region_emb_128_9_cw_short_global/best
    # restore_checkpoint: /mnt/cephfs/lab/chenxinhao.0/checkpoint/example/conf/wukong5_bert/no_sigmoid_3_25_merge_shuf_online_single_fc.yaml/best_accuracy_in_online_extend_dev
    # auto_end_time: 10000
    # skip_eval_save: True