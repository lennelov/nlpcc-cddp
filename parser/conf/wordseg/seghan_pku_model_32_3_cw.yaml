default_dataset:
  buffer_size: 20000
  delim: "\t"
  map_tables: &id001
    gram2id: {path: data/wordseg/sighan05_pku/gram2id.dict}
    tag2id: {path: data/wordseg/sighan05_pku/label2id.dict}
    token2id: {path: data/wordseg/sighan05_pku/char2id1.dict}
  shuffle: true
  slots: &id002
    grams: {delim: ' ', index: 3, map_table: gram2id, max_length: 1019, pad: 0, type: sequence}
    nwords: {index: 2, type: value, value_type: int64}
    tags: {delim: ' ', index: 1, map_table: tag2id, max_length: 1019, pad: 0, type: sequence}
    tokens: {delim: ' ', index: 0, map_table: token2id, max_length: 1019, pad: 0,
      type: sequence}
  type: PythonDataset
dev_dataset: &id003
  buffer_size: 54000
  delim: "\t"
  map_tables: *id001
  path: data/wordseg/sighan05_pku/test.data
  shuffle: false
  slots: *id002
  type: PythonDataset
estimator:
  batch_size: 200
  checkpoint_dir: results/wordseg_pku/region_emb_32_3_cw
  display_eval: true
  eval_datasets:
  - &id004
    buffer_size: 54000
    delim: "\t"
    map_tables: *id001
    path: data/wordseg/sighan05_pku/train.data
    shuffle: true
    slots: *id002
    type: PythonDataset
  - *id003
  eval_interval_steps: 100
  eval_op_path: results/wordseg_pku/region_emb_32_3_cw/summary/eval.output
  infer2id: data/wordseg/sighan05_pku/infer2id.dict
  label2id: data/wordseg/sighan05_pku/label2id.dict
  log_every_n_steps: 100
  max_epochs: 20
  max_training_steps: 4000
  model_name: region_sequence_labbelling_model
  save_checkpoints_steps: 2000
  train_dataset: *id004
  type: PythonSequenceLabellingEstimator
  use_crf: true
  use_entity_f1: false
  word2id: data/wordseg/sighan05_pku/char2id1.dict
logging: {file: results/wordseg_pku/region_emb_32_3_cw/logging.out, level: 3}
model: {batch_size: 200, context_word: false, dropout_rate: 0.3, emb_size: 32, fc_in: 64,
  gram_emb_size: 32, gram_size: 131333, hidden_size: 256, learning_rate: 0.005, max_length: 1019,
  metric: NERMetric, model_name: RegionChunkingModel, n_classes: 6, optimizer: Adam,
  region_size: 3, use_crf: true, use_word_pretrain_emb: false, vocab_size: 4687}
pred_dataset:
  buffer_size: 54000
  delim: "\t"
  map_tables: &id005
    gram2id: {path: data/wordseg/sighan05_pku/gram2id.dict}
    tag2id: {path: data/wordseg/sighan05_pku/label2id.dict}
    token2id: {path: data/wordseg/sighan05_pku/char2id1.dict}
  shuffle: true
  slots: &id006
    grams: {delim: ' ', index: 2, map_table: gram2id, max_length: 1019, pad: 0, type: sequence}
    nwords: {index: 1, type: value, value_type: int64}
    tags: {delim: ' ', index: 0, map_table: token2id, max_length: 1019, pad: 0, type: sequence}
    tokens: {delim: ' ', index: 0, map_table: token2id, max_length: 1019, pad: 0,
      type: sequence}
  type: PythonDataset
predict_dataset:
  buffer_size: 54000
  delim: "\t"
  map_tables: *id005
  path: data/wordseg/sighan05_pku/test.data
  shuffle: false
  slots: *id006
  type: PythonDataset
test_dataset:
  buffer_size: 54000
  delim: "\t"
  map_tables: *id001
  path: data/wordseg/sighan05_pku/test.data
  shuffle: false
  slots: *id002
  type: PythonDataset
train_dataset: *id004
