---

# Dataset metadata
default_dataset: &default_dataset
    type: PythonDataset
    delim: "\t"
    buffer_size: 5000
    shuffle: True
    map_tables:
        word2id:
            path: data/word2id.table # word2id.random.table if random embedding
        pos2id:
            path: data/pos2id.table

    slots: 
        idx: &default_slot
            index: 0
            type: sequence
            delim: " "
            value_type: int64
            max_length: 240
            pad: 0
        word: &seq_slot
            index: 1
            type: sequence
            delim: " "
            map_table: word2id
            max_length: 240
            pad: 0
        upos:
            index: 2
            map_table: pos2id
            << : *seq_slot
        xpos:
            index: 3
            map_table: pos2id
            << : *seq_slot
        head:
            index: 4
            << : *default_slot
        nwords:
            index: 5
            type: value
            value_type: int64
            

pred_dataset: &pred_dataset
    type: PythonDataset
    delim: "\t"
    buffer_size: 2000
    shuffle: True
    map_tables:
        word2id:
            path: data/word2id.table # word2id.random.table if random embedding
        pos2id:
            path: data/pos2id.table

    slots: 
        idx:
            index: 0
            << : *default_slot
        word:
            index: 1
            map_table: word2id
            << : *seq_slot
        upos:
            index: 2
            map_table: pos2id
            << : *seq_slot
        xpos:
            index: 3
            map_table: pos2id
            << : *seq_slot
        nwords:
            index: 4
            type: value
            value_type: int64

train_dataset: &train_dataset
    path: data/train.new
    << : *default_dataset

dev_dataset: &dev_dataset
    shuffle: False
    path: data/dev.new
    << : *default_dataset

test_dataset: &test_dataset
    shuffle: False
    path: data/test.new
    << : *default_dataset

predict_dataset: &predict_dataset
    shuffle: False
    path: data/test.new
    << : *pred_dataset

# logging:
#     file: results/dependency_parsing/logging.out
#     level: 3

# Model config
model:
    model_name: DependencyModel
    use_word_pretrain_emb: False
    word_emb_finetune: False
    word2vec_dict: 'data/embedding/emb.50' 
    vocab_size: 12446 # 8679 if random embedding
    pos_size: 43

    use_crf: False
    max_length: 240
    emb_size: 50
    batch_size: 2
    optimizer: Adam
    learning_rate: 0.005
    use_clip_by_norm: True
    decay_step: 300
    decay_rate: 0.9
    dropout_rate: 0.3
    n_classes: 200
    hidden_size: 128
    fc_hidden_size: 256
    metric: NERMetric

# Estimator
estimator:
    type: PythonEstimator
    train_dataset: *train_dataset
    eval_datasets:
        # - *train_dataset
        - *dev_dataset
        # - *test_dataset
    eval_to_file: True
    eval_op_path: results/dependency_parsing/eval.output
    # infer_dataset: *predict_dataset
    # infer_op_path: results/dependency_parsing/infer.output

    batch_size: 2
    max_epochs: 40

    use_crf: False
    chunking: False
    use_entity_f1: False
    eval_with_input: False
    word2id: data/word2id.table # word2id.random.table if random embedding
    pos2id: data/pos2id.table
    checkpoint_dir: results/dependency_parsing/checkpoint

    model_name: dependency_parsing_model
    # save_checkpoints_steps: 2000
    # eval_interval_epochs: 100
    eval_interval_steps: 1
    max_training_steps: 400000
    log_every_n_steps: 1
    tolerance: 5
    skip_eval_save: False